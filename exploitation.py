import pymongo
import pandas as pd
from pyspark.sql import SparkSession, SQLContext, Row
from pyspark import SparkConf, SparkContext
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import sum, col


client_mongo = pymongo.MongoClient("mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/?retryWrites=true&w=majority")
sc = SparkContext()
spark = SparkSession(sc)

#spark = SparkSession.builder.appName("mongodbtest1").master('local').config("spark.mongodb.input.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/?retryWrites=true&w=majority/all.income").config("spark.mongodb.output.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/?retryWrites=true&w=majority/all.income").config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1').getOrCreate()

# TRIED TO READ DIRECTLY FROM MONGO TO A PYSPARK DATAFRAME
# spark = SparkSession.builder \
#     .appName("MongoDBSparkConnector") \
#     .config("spark.mongodb.input.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/all.income?authSource=admin") \
#     .config("spark.mongodb.output.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/all.income?authSource=admin") \
#     .getOrCreate()
# # Read data from MongoDB into a DataFrame
# data = spark.read.format("mongodb").load()


if __name__ == "__main__":
# Database Name
    db = client_mongo["all"]
    
    # Collection Name
    col = db["income"] # read income collection

    df_income = pd.DataFrame(list(col.find()))  # convert it to pandas dataframe
    df_income.drop(['_id'], axis=1, inplace=True)   # drop _id column
    income = spark.createDataFrame(df_income)   # convert it to pyspark

    col = db["incidents"]

    # similar for incidents collection
    df_incidents = pd.DataFrame(list(col.find()))
    df_incidents.drop(['_id'], axis=1, inplace=True)
    df_incidents = df_incidents[df_incidents['NK any'].notna()]     # keep only rows where year is not na
    df_incidents['NK any'] = df_incidents['NK any'].astype(float) + 7   # add 7 years to each row so we have some values for 2020 2021 (synthetically)
    incidents = spark.createDataFrame(df_incidents)

    col = db["idealista"]
    # similar with income do it for idealista collection
    df_idealista = pd.DataFrame(list(col.find()))
    df_idealista.drop(['_id'], axis=1, inplace=True)
    idealista = spark.createDataFrame(df_idealista)


    # For all 3 dataframes keep only rows with year 2020 and 2021
    income1 = income.where((income.year =='2020') | (income.year =='2021'))
    incidents1 = incidents.where((incidents['NK any'] =='2020') | (incidents['NK any']=='2021'))
    idealista1 = idealista.where((idealista.year =='2020') | (idealista.year =='2021'))

    # from idealista keep only useful columns (after checking their values count)
    idealista2 = idealista.select([c for c in idealista.columns if c in ['bathrooms', 'distance','exterior','floor','hasLift','hasPlan','hasVideo','municipality','neighborhood','numPhotos','price','priceByArea','propertyType','rooms','size','status','year']])

    # from the columns kept in idealista, floor, hasLift and status have values None and nan. Drop all rows with these values
    idealista3 = idealista2.where((idealista2.floor != 'None') & (idealista2.floor != 'nan') & (idealista2.hasLift != 'None') & (idealista2.hasLift != 'nan')
                               & (idealista2.status!= 'None') & (idealista2.status != 'nan'))
    
    # for incidents we want to groupby neighborhood and year and sum all incidents so we have total number of incidents that happened in a neighborhood for years 2020 and 2021.
    incidents2 = incidents1.withColumn("Número d'incidents GUB", incidents1["Número d'incidents GUB"].cast(IntegerType()))
    incidents3 = incidents2.groupBy(["Nom barri", "NK any"]).agg(sum("Número d'incidents GUB").alias("nof_incidents"))

    # print((idealista3.count(), len(idealista3.columns)))
    # We want to join all the dataframes into one. First join idealista and incidents by neighborhood and year. Keep all idealista columns and from incidents only the new grouped by feature with nof incidents 
    df = idealista3.join(incidents3, (idealista3.neighborhood == incidents3['Nom barri']) & (idealista3.year == incidents3['NK any'])).select(idealista3['*'],incidents3['nof_incidents'])
    # it seems that the reconciliation did not happen. They have different neighborhoods. I dont know if this is only in idealista or in every dataset.
    # print(income1.show())
    # Then join this dataframe with income, by neighborhood and year again. Keep all columns from dataframe and population and RFD from income dataframe.
    df2 = df.join(income1, (df.neighborhood == income1['neigh_name']) & (df.year == income1['year'])).select(df['*'],income1['pop'],income1['RFD'])
    df2.show()




# SEE VALUE COUNTS OF THE COLUMNS (I CHECKED ALL OF THEM) TO SEE WHICH ARE USEFUL TO KEEP
# print((df.count(), len(df.columns)))
# idealista.groupBy('municipality').count().orderBy('count').show()
# idealista.groupBy('propertyType').count().orderBy('count').show()
#idealista.groupBy('distance').count().orderBy('count', ascending=False).show()
#idealista.groupBy('numPhotos').count().orderBy('count', ascending=False).show()
# idealista.groupBy('topNewDevelopment').count().orderBy('count').show()


# THIS CODE COUNTS THE NA VALUES IN THE PYSPARK DATAFRAME
# nof_rows = idealista2.count()
# for c in idealista2.columns:
#     count_na= idealista2.where((idealista2[c] == 'None') | (idealista2[c] == 'nan')).count()  # when converting to spark df, the NaN/None values are becoming string values.
#     print(c, count_na/nof_rows)
