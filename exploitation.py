import pymongo
import pandas as pd
from pyspark.sql import SparkSession, SQLContext, Row
from pyspark import SparkConf, SparkContext
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import sum, col
import json
import logging


client_mongo = pymongo.MongoClient("mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/?retryWrites=true&w=majority")
sc = SparkContext()
spark = SparkSession(sc)
logging.getLogger().setLevel(logging.INFO)
logging.basicConfig(filename='logs/exploitation.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')

#spark = SparkSession.builder.appName("mongodbtest1").master('local').config("spark.mongodb.input.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/?retryWrites=true&w=majority/all.income").config("spark.mongodb.output.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/?retryWrites=true&w=majority/all.income").config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1').getOrCreate()

# TRIED TO READ DIRECTLY FROM MONGO TO A PYSPARK DATAFRAME
# spark = SparkSession.builder \
#     .appName("MongoDBSparkConnector") \
#     .config("spark.mongodb.input.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/all.income?authSource=admin") \
#     .config("spark.mongodb.output.uri", "mongodb+srv://alextrem:1234@cluster0.x1yh6no.mongodb.net/all.income?authSource=admin") \
#     .getOrCreate()
# # Read data from MongoDB into a DataFrame
# data = spark.read.format("mongodb").load()

def store_to_mongo(client, source, collection_name, df):
        db = client[source]
        collection = db[collection_name]
        results = df.toJSON().map(lambda j: json.loads(j)).collect()
        for res in results:
            collection.insert_one(res)
            # print(res)


if __name__ == "__main__":
    logging.info("Starting Explotation zone data creation.")
    logging.info("Starting Extraction of data from Formatted zone.")
    # Database Name
    db = client_mongo["all"]
    
    logging.info("Starting Extraction of income data.")
    # Collection Name
    col = db["income"] # read income collection

    df_income = pd.DataFrame(list(col.find()))  # convert it to pandas dataframe
    df_income.drop(['_id'], axis=1, inplace=True)   # drop _id column
    income = spark.createDataFrame(df_income)   # convert it to pyspark
    logging.info("Successfully extracted income data.")
    logging.info("Starting Extraction of incidents data.")
    col = db["incidents"]

    # similar for incidents collection
    df_incidents = pd.DataFrame(list(col.find()))
    df_incidents.drop(['_id'], axis=1, inplace=True)
    incidents = spark.createDataFrame(df_incidents)
    logging.info("Successfully extracted incidents data.")
    logging.info("Starting Extraction of idealista data.")
    col = db["idealista"]
    # similar with income do it for idealista collection
    df_idealista = pd.DataFrame(list(col.find()))
    df_idealista.drop(['_id'], axis=1, inplace=True)
    idealista = spark.createDataFrame(df_idealista)
    logging.info("Successfully extracted idealista data.")

    logging.info("Starting preprocessing of the data sources.")
    logging.info("Filtering by selected years: 2020 and 2021.")
    # For all 3 dataframes keep only rows with year 2020 and 2021
    income1 = income.where((income.year =='2020') | (income.year =='2021'))
    incidents1 = incidents.where((incidents['NK_any'] =='2020') | (incidents['NK_any']=='2021'))
    idealista1 = idealista.where((idealista.year =='2020') | (idealista.year =='2021'))
    idealista1b = idealista1.where((idealista1.municipality =='Barcelona'))

    logging.info("Selecting useful columns.")
    # from idealista keep only useful columns (after checking their values count)
    idealista2 = idealista1b.select([c for c in idealista1b.columns if c in ['bathrooms', 'distance','exterior','floor','hasLift','hasPlan','hasVideo','neighborhood','numPhotos','price','propertyType','rooms','size','status','year']])

    logging.info("Imputing null in floor by the average floor of the dataframe.")
    # Impute the null in floor by the average floor of the dataframe 
    idealista2a = idealista2.withColumn("floor", idealista2["floor"].cast(IntegerType()))  # first convert it to int. All values that cannot be converted to nums like 'Nan' and 'None' become NULL

    avgFloor = idealista2a.agg({'floor': 'avg'}).collect()  # calculate the average and store it
    results={}
    for i in avgFloor:
        results.update(i.asDict())
    avgFloor = results['avg(floor)']    # extract the actual avg value

    idealista2b = idealista2a.fillna({'floor':avgFloor})    # impute nas in floor with the avg
    idealista2c = idealista2b.withColumn("floor", idealista2b["floor"].cast(IntegerType()))     # convert back to integers to perform rounding sicne the average was a float.

    logging.info("Dropping unnecessary columns of idealista.")
    # from the columns kept in idealista, floor, hasLift and status have values None and nan. Drop all rows with these values
    idealista3 = idealista2c.where((idealista2c.hasLift != 'None') & (idealista2c.hasLift != 'nan')
                               & (idealista2c.status!= 'None') & (idealista2c.status != 'nan'))

    logging.info("Aggregating incidents at desired level.")
    # for incidents we want to groupby neighborhood and year and sum all incidents so we have total number of incidents that happened in a neighborhood for years 2020 and 2021.
    incidents2 = incidents1.withColumn("Numero_incidents_GUB", incidents1["Numero_incidents_GUB"].cast(IntegerType()))
    incidents3 = incidents2.groupBy(["Nom_barri", "NK_any"]).agg(sum("Numero_incidents_GUB").alias("nof_incidents"))

    logging.info("Joining dataframes.")
    df = idealista3.join(incidents3, (idealista3.neighborhood == incidents3['Nom_barri']) & (idealista3.year == incidents3['NK_any'])).select(idealista3['*'],incidents3['nof_incidents'])
    df2 = df.join(income1, (df.neighborhood == income1['neigh_name']) & (df.year == income1['year'])).select(df['*'],income1['pop'],income1['RFD'])

    logging.info("Replacing categorical values by integers.")
    statusDic={'newdevelopment':'1','renew':'2','good':'3'}
    df3 = df2.replace(statusDic,subset=['status'])
    hasLiftDic={'True':'1','False':'0'}     # since the values are True/False and not true/false they will not converted to 1/0 with integer casting. So, we have to replace the values mannualy
    df4 = df3.replace(hasLiftDic,subset=['hasLift'])

    exclude_columns = ["neighborhood", "propertyType"]
    # Select and cast the appropriate columns
    df5 = df4.select([df4[c].cast("integer").alias(c) if c not in exclude_columns else df4[c] for c in df4.columns])
    df5.show()

    logging.info("Successfully finished preprocessing data.")

    try:
        logging.info("Starting uploading data to exploitation zone.")
        store_to_mongo(client_mongo, 'exploitation', "predictiveKPI_salePrice", df5)
        logging.info("Successfully uploaded data to exploitation zone.")

    except ConnectionResetError as e:
        logging.critical(f"ConnectionResetError ocurred: {e}.")
    except Exception as e:
        logging.critical(f"An unexpected error occurred: {e}.")
           
    client_mongo.close()
    spark.stop()

# SEE VALUE COUNTS OF THE COLUMNS (I CHECKED ALL OF THEM) TO SEE WHICH ARE USEFUL TO KEEP
# print((df.count(), len(df.columns)))
# idealista.groupBy('municipality').count().orderBy('count').show()
# idealista.groupBy('propertyType').count().orderBy('count').show()
#idealista.groupBy('distance').count().orderBy('count', ascending=False).show()
#idealista.groupBy('numPhotos').count().orderBy('count', ascending=False).show()
# idealista.groupBy('topNewDevelopment').count().orderBy('count').show()


# THIS CODE COUNTS THE NA VALUES IN THE PYSPARK DATAFRAME
# nof_rows = idealista2.count()
# for c in idealista2.columns:
#     count_na= idealista2.where((idealista2[c] == 'None') | (idealista2[c] == 'nan')).count()  # when converting to spark df, the NaN/None values are becoming string values.
#     print(c, count_na/nof_rows)